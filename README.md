# NLP-Word-Embedding-Models
æ­¤ç‚ºæ¸…å¤§è‡ªç„¶èªè¨€è™•ç†èª²ç¨‹çš„ HW1: Practice word analogy (Google Analogy dataset) via 

(1) the pre-trained word embedding and also 

(2) the new word embedding trained by yourself.

# NLP Homework 1 Report

**Platform:** Local  
**Python version:** 3.10.18  
**Operating system:** Linux 6.8.0-65-generic  
**CPU:** x86_64 24æ ¸å¿ƒ  
**GPU:** NVIDIA GeForce RTX 3090  

---

## 1. Embedding Model & Preprocessing (5%)

**ä½¿ç”¨çš„è©åµŒå…¥æ¨¡å‹**  
- é è¨“ç·´æ¨¡å‹ï¼šGloVeï¼ˆ`glove-wiki-gigaword-300`ï¼‰  
- è‡ªè¨“ç·´æ¨¡å‹ï¼šä½¿ç”¨ 20% æŠ½æ¨£ç¶­åŸºç™¾ç§‘æ–‡ç« è¨“ç·´çš„ Word2Vec  

**é è™•ç†æ­¥é©Ÿ**  
- åŸºæœ¬è™•ç†ï¼šå°å¯«åŒ–ã€ç§»é™¤æ¨™é»ç¬¦è™Ÿ/ç‰¹æ®Šå­—å…ƒã€ç§»é™¤æ•¸å­—ã€éæ¿¾çŸ­è©ã€ç©ºç™½æ­£è¦åŒ–  
- æ”¹è‰¯è™•ç†ï¼š  
  - ASCII éæ¿¾ç§»é™¤éè‹±æ–‡è©  
  - ç§»é™¤åœç”¨è©ï¼ˆNLTK stopwordsï¼‰  
  - è©å½¢é‚„åŸï¼ˆWordNetLemmatizerï¼‰  
  - ç²¾ç¢ºæ–·è©ï¼ˆ`nltk.word_tokenize`ï¼‰  
  - æœ€å°å¥é•· â‰¥ 5  

**Word2Vec è¶…åƒæ•¸**  
- `vector_size=300, window=5, min_count=5, workers=4, sg=0, epochs=10`  
- `negative=5, sample=1e-3, seed=4321`  

---

## 2. Performance on Different Sampling Ratios (10%)

**æŠ½æ¨£è³‡æ–™**  
- 5% â†’ 281,253 ç¯‡æ–‡ç«   
- 10% â†’ 563,068 ç¯‡æ–‡ç«   
- 20% â†’ 1,123,661 ç¯‡æ–‡ç«   

**é æœŸè¡¨ç¾**  
- **5%**ï¼šè©å½™è¦†è“‹ç‡ 60-70%ï¼Œé¡æ¯”æº–ç¢ºç‡ 15-25%ï¼Œè¨“ç·´æ™‚é–“æœ€çŸ­ä½†èªç¾©ä¸å®Œæ•´  
- **10%**ï¼šè©å½™è¦†è“‹ç‡ 75-85%ï¼Œæº–ç¢ºç‡ 25-35%ï¼Œæ•ˆèƒ½èˆ‡è³‡æºå¹³è¡¡  
- **20%**ï¼šè©å½™è¦†è“‹ç‡ 85-90%ï¼Œæº–ç¢ºç‡ 35-45%ï¼Œæœ€ä½³èªç¾©æ•æ‰ï¼Œä½†è¨“ç·´æ™‚é–“æœ€é•·  

â¡ æ•ˆèƒ½éš¨æŠ½æ¨£æ¯”ä¾‹éå¢ï¼Œä½† **é‚Šéš›æ•ˆç›Šéæ¸›**ã€‚  

---

## 3. Corpus Comparison (15%)

### 3.1 Results
| èªæ–™ | æè¿° | æº–ç¢ºç‡ | è©å½™è¦†è“‹ç‡ | è¨“ç·´æ™‚é–“ |
|------|------|--------|------------|----------|
| Wikipedia 20% | ç™¾ç§‘å…¨æ›¸æ–‡æœ¬ | **58.80%** | 99.77% | 311s |
| News Corpus  | æ–°èèªæ–™ | 0.00% | 3.57% | 1s |
| Literature Corpus | æ–‡å­¸èªæ–™ | 0.03% | 11.13% | 2.9s |

**èªç¾©é¡æ¯” (Semantic)**  
- Wiki: 62.24% (839/1348)  
- News: 0.00% (0/13)  
- Literature: 0.00% (0/27)  

**å¥æ³•é¡æ¯” (Syntactic)**  
- Wiki: 56.23% (925/1645)  
- Literature: 0.33% (1/307)  
- News: 0.00% (0/94)  

### 3.2 å·®ç•°èªªæ˜
- **Wikipedia**ï¼šå¤§è¦æ¨¡ (~80k å¥)ã€æ­£å¼ã€ç™¾ç§‘å¼çŸ¥è­˜ â†’ é«˜å¤šæ¨£æ€§è©å½™  
- **News**ï¼šå°èªæ–™ï¼Œæ™‚äº‹æ”¿æ²»åå¤šï¼Œèªè¨€å£èªåŒ–  
- **Literature**ï¼šå°èªæ–™ï¼Œæƒ…æ„Ÿèˆ‡ä¿®è¾­å¤šï¼Œèªè¨€é¢¨æ ¼ç‰¹æ®Š  

### 3.3 åŸå› åˆ†æ
- Wikipedia æº–ç¢ºç‡é«˜ â†’ è¦†è“‹å»£ã€èªæ–™å¤§ã€å…§å®¹ä¸€è‡´æ€§å¼·  
- News/Literature æº–ç¢ºç‡ä½ â†’ è³‡æ–™å°‘ã€é ˜åŸŸåå·®ã€é¢¨æ ¼å·®ç•°å¤§  

---

## 4. Word Similarity Results (10%)

### ä¾‹å­ï¼šTop-5 ç›¸ä¼¼è©
- **king** â†’ queen, prince, monarch, royal, kingdom  
- **computer** â†’ technology, software, digital, machine, electronic  
- **father** â†’ mother, parent, son, family, dad  
- **science** â†’ research, scientific, technology, knowledge, study  
- **love** â†’ passion, emotion, heart, relationship, feeling  

**è§€å¯Ÿ**  
- èªç¾©èšé¡æ•ˆæœæ˜é¡¯ï¼ˆé ˜åŸŸè©æœƒèšåœ¨ä¸€èµ·ï¼‰  
- è©æ€§ä¸€è‡´æ€§ï¼šåè©å°åè©ã€å‹•è©å°å‹•è©  
- å­¸åˆ°åŒç¾©ã€ä¸Šä¸‹ä½ã€æ€§åˆ¥å°æ‡‰ç­‰èªç¾©é—œä¿‚  

---

## 5. Suggestions for Strengthening Report (5%)

- **æ›´å¤šæŒ‡æ¨™**ï¼šPrecision/Recall/F1-score  
- **è¦–è¦ºåŒ–**ï¼šå­¸ç¿’æ›²ç·šã€æ··æ·†çŸ©é™£ã€è©å½™è¦†è“‹ç†±åœ–  
- **å¯è§£é‡‹æ€§**ï¼šæ³¨æ„åŠ›åˆ†å¸ƒã€PCA ç¶­åº¦åˆ†æ  
- **å°æ¯”æ¨¡å‹**ï¼šèˆ‡ BERTã€GPT ç­‰ä¸Šä¸‹æ–‡åµŒå…¥æ¯”è¼ƒ  

---

ğŸ“Œ *This README is a summary of NLP HW1 report.*
