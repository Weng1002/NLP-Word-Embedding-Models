{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f941b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.24.3\n",
    "!pip install pandas==2.0.3\n",
    "!pip install scikit-learn==1.3.0\n",
    "!pip install matplotlib==3.7.2\n",
    "!pip install gensim==4.3.1\n",
    "!pip install tqdm==4.65.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae73266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b6eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://download.tensorflow.org/data/questions-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaa7d6a",
   "metadata": {},
   "source": [
    "# Part I: Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31700f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "file_name = \"questions-words\"\n",
    "with open(f\"{file_name}.txt\", \"r\") as f:\n",
    "    data = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e6e4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": capital-common-countries\n",
      "Athens Greece Baghdad Iraq\n",
      "Athens Greece Bangkok Thailand\n",
      "Athens Greece Beijing China\n",
      "Athens Greece Berlin Germany\n",
      "Athens Greece Bern Switzerland\n",
      "Athens Greece Cairo Egypt\n",
      "Athens Greece Canberra Australia\n",
      "Athens Greece Hanoi Vietnam\n",
      "Athens Greece Havana Cuba\n"
     ]
    }
   ],
   "source": [
    "# check data from the first 10 entries\n",
    "for entry in data[:10]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b498634",
   "metadata": {},
   "source": [
    "語義類比類別 (5)：\n",
    "\n",
    "- capital-common-countries: 常見國家首都關係\n",
    "\n",
    "- capital-world: 世界各國首都關係\n",
    "\n",
    "- city-in-state: 城市與州的關係\n",
    "\n",
    "- currency: 貨幣關係\n",
    "\n",
    "- family: 家庭關係\n",
    "\n",
    "句法類比類別 (9)：\n",
    "\n",
    "- gram1-adjective-to-adverb: 形容詞轉副詞\n",
    "- gram2-opposite: 反義詞\n",
    "\n",
    "- gram3-comparative: 比較級\n",
    "\n",
    "- gram4-superlative: 最高級\n",
    "\n",
    "- gram5-present-participle: 現在分詞\n",
    "\n",
    "- gram6-nationality-adjective  國籍形容詞 \n",
    "\n",
    "- gram7-past-tense  過去時態         \n",
    "\n",
    "- gram8-plural  複數              \n",
    "\n",
    "- gram9-plural-verbs   複數動詞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2f320",
   "metadata": {},
   "source": [
    "## TODO 1\n",
    "\n",
    "1. 讀取檔案行，分辨哪些是章節標頭，哪些是題目資料。\n",
    "\n",
    "2. 記錄章節資訊：\n",
    "\n",
    "    檔案中前 5 個 \": \" 開頭的章節 → 標記為 semantic\n",
    "\n",
    "    之後的 9 個章節 → 標記為 syntactic\n",
    "\n",
    "3. 把 analogy 四元組轉成結構化格式：\n",
    "\n",
    "    欄位設計：\n",
    "\n",
    "        section（章節名稱，例如 capital-common-countries）\n",
    "\n",
    "        type（semantic / syntactic）\n",
    "\n",
    "        a, b, c, d（四個詞，對應 a:b :: c:d）\n",
    "\n",
    "4. 最後存成 DataFrame，方便後續做統計或送進 word2vec 測試。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8cc934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總類比數量: 19544\n",
      "語義類比: 8869\n",
      "句法類比: 10675\n",
      "子類別數: 14\n",
      "\n",
      "前幾行:\n",
      "                         Question  Category               SubCategory\n",
      "0      Athens Greece Baghdad Iraq  semantic  capital-common-countries\n",
      "1  Athens Greece Bangkok Thailand  semantic  capital-common-countries\n",
      "2     Athens Greece Beijing China  semantic  capital-common-countries\n",
      "3    Athens Greece Berlin Germany  semantic  capital-common-countries\n",
      "4  Athens Greece Bern Switzerland  semantic  capital-common-countries\n",
      "\n",
      "類別分佈:\n",
      "Category   SubCategory                \n",
      "semantic   capital-common-countries        506\n",
      "           capital-world                  4524\n",
      "           city-in-state                  2467\n",
      "           currency                        866\n",
      "           family                          506\n",
      "syntactic  gram1-adjective-to-adverb       992\n",
      "           gram2-opposite                  812\n",
      "           gram3-comparative              1332\n",
      "           gram4-superlative              1122\n",
      "           gram5-present-participle       1056\n",
      "           gram6-nationality-adjective    1599\n",
      "           gram7-past-tense               1560\n",
      "           gram8-plural                   1332\n",
      "           gram9-plural-verbs              870\n",
      "dtype: int64\n",
      "\n",
      "DataFrame形狀: (19544, 3)\n",
      "欄位名稱: ['Question', 'Category', 'SubCategory']\n",
      "\n",
      "資料已儲存至 questions-words.csv\n"
     ]
    }
   ],
   "source": [
    "# TODO1: 處理資料為pd.DataFrame的程式碼\n",
    "# 請注意前五個\": \"表示語義類別(semantic)，\n",
    "# 其餘九個屬於句法類別(syntactic)\n",
    "\n",
    "questions = []          \n",
    "categories = []        \n",
    "sub_categories = []    \n",
    "\n",
    "current_category = None\n",
    "category_count = 0\n",
    "category_type = None\n",
    "\n",
    "for line in data:\n",
    "    if line.startswith(\": \"):\n",
    "        current_category = line[2:]  # 移除 \": \" 前綴\n",
    "        category_count += 1\n",
    "        \n",
    "        # 根據順序確定是語義還是句法類別\n",
    "        if category_count <= 5:\n",
    "            category_type = \"semantic\"\n",
    "        else:\n",
    "            category_type = \"syntactic\"\n",
    "    else:\n",
    "        # 這是類比行\n",
    "        if current_category and line.strip():  # 確保有類別且行不為空\n",
    "            words = line.split()\n",
    "            if len(words) == 4:  #\n",
    "                # 將四個詞組合成一個問題字串\n",
    "                question = f\"{words[0]} {words[1]} {words[2]} {words[3]}\"\n",
    "                \n",
    "                questions.append(question)\n",
    "                categories.append(category_type)\n",
    "                sub_categories.append(current_category)\n",
    "\n",
    "# 創建DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Question\": questions,\n",
    "        \"Category\": categories,\n",
    "        \"SubCategory\": sub_categories,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 顯示資料集的基本資訊\n",
    "print(f\"總類比數量: {len(df)}\")\n",
    "print(f\"語義類比: {len(df[df['Category'] == 'semantic'])}\")\n",
    "print(f\"句法類比: {len(df[df['Category'] == 'syntactic'])}\")\n",
    "print(f\"子類別數: {df['SubCategory'].nunique()}\")\n",
    "print(\"\\n前幾行:\")\n",
    "print(df.head())\n",
    "print(\"\\n類別分佈:\")\n",
    "print(df.groupby(['Category', 'SubCategory']).size())\n",
    "\n",
    "# 顯示DataFrame結構\n",
    "print(f\"\\nDataFrame形狀: {df.shape}\")\n",
    "print(f\"欄位名稱: {list(df.columns)}\")\n",
    "\n",
    "df.to_csv(f\"{file_name}.csv\", index=False)\n",
    "print(f\"\\n資料已儲存至 {file_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ba867f",
   "metadata": {},
   "source": [
    "# Part II: Use pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db7dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gensim model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"questions-words.csv\")\n",
    "\n",
    "MODEL_NAME = \"glove-wiki-gigaword-300\"\n",
    "# You can try other models.\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models\n",
    "\n",
    "# 載入預訓練模型（這裡使用GloVe向量）\n",
    "model = gensim.downloader.load(MODEL_NAME)\n",
    "print(\"The Gensim model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd978c",
   "metadata": {},
   "source": [
    "## TODO 2\n",
    "\n",
    "1. 類比解析：將問題字串分割成四個詞 word_a word_b word_c word_d\n",
    "\n",
    "2. 向量運算：\n",
    "\n",
    "- 使用公式：word_b - word_a + word_c ≈ word_d\n",
    "- 在gensim中：positive=[word_b, word_c], negative=[word_a]\n",
    "\n",
    "3. 預測策略：\n",
    "\n",
    "- 取前10個最相似的詞\n",
    "- 排除輸入的三個詞（避免trivial答案）\n",
    "- 選擇第一個有效的候選詞\n",
    "\n",
    "4. 錯誤處理：\n",
    "\n",
    "- 處理詞彙表外的詞（OOV）\n",
    "- 處理其他可能的異常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa580d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do predictions and preserve the gold answers (word_D)\n",
    "# 進行預測並保存正確答案 (word_D)\n",
    "preds = []\n",
    "golds = []\n",
    "\n",
    "for analogy in tqdm(data[\"Question\"]):\n",
    "    # TODO2: Write your code here to use pre-trained word embeddings for getting predictions of the analogy task.\n",
    "    # You should also preserve the gold answers during iterations for evaluations later.\n",
    "    # TODO2: 使用預訓練詞嵌入進行類比任務預測的程式碼\n",
    "    # 您也應該在迭代過程中保存正確答案以便後續評估\n",
    "    # 解析類比問題 (例如: \"man woman king queen\")\n",
    "    \"\"\" Hints\n",
    "    # Unpack the analogy (e.g., \"man\", \"woman\", \"king\", \"queen\")\n",
    "    # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d\n",
    "    # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776\n",
    "    # Mikolov et al., 2013: big - biggest and small - smallest\n",
    "    # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).\n",
    "    \"\"\"\n",
    "      \n",
    "    words = analogy.split()\n",
    "    word_a, word_b, word_c, word_d = words\n",
    "    \n",
    "    # GloVe模型的詞彙表都是小寫的!!!!\n",
    "    word_a_lower = word_a.lower()\n",
    "    word_b_lower = word_b.lower()\n",
    "    word_c_lower = word_c.lower()\n",
    "    word_d_lower = word_d.lower()\n",
    "    \n",
    "    # 保存正確答案\n",
    "    golds.append(word_d_lower)\n",
    "\n",
    "    if all(word in model.key_to_index for word in [word_a_lower, word_b_lower, word_c_lower]):\n",
    "        # 執行向量運算: word_b - word_a + word_c ≈ word_d\n",
    "        # 例如: queen = king - man + woman\n",
    "        result = model.most_similar(\n",
    "                positive=[word_b_lower, word_c_lower],  # woman, king\n",
    "                negative=[word_a_lower],                # man\n",
    "                topn=10\n",
    "        )\n",
    "        \n",
    "        # 找到第一個不是輸入詞的預測結果\n",
    "        prediction = None\n",
    "        for candidate, similarity in result:\n",
    "            if candidate not in [word_a_lower, word_b_lower, word_c_lower]:\n",
    "                prediction = candidate\n",
    "                break\n",
    "        \n",
    "        # 如果找到預測結果，則使用它；否則使用最相似的詞\n",
    "        if prediction is None:\n",
    "            prediction = result[0][0]\n",
    "            \n",
    "        preds.append(prediction)\n",
    "        \n",
    "    else:\n",
    "        # 如果有詞不在詞彙表中，預測為空字串或特殊標記\n",
    "        preds.append(\"<UNK>\")\n",
    "\n",
    "print(f\"完成！處理了 {len(preds)} 個類比問題\")\n",
    "print(f\"預測範例: {preds[:5]}\")\n",
    "print(f\"正確答案範例: {golds[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e81e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform evaluations. You do not need to modify this block!!\n",
    "\n",
    "def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:\n",
    "    return np.mean(gold == pred)\n",
    "\n",
    "golds_np, preds_np = np.array(golds), np.array(preds)\n",
    "data = pd.read_csv(\"questions-words.csv\")\n",
    "\n",
    "# Evaluation: categories\n",
    "for category in data[\"Category\"].unique():\n",
    "    mask = data[\"Category\"] == category\n",
    "    golds_cat, preds_cat = golds_np[mask], preds_np[mask]\n",
    "    acc_cat = calculate_accuracy(golds_cat, preds_cat)\n",
    "    print(f\"Category: {category}, Accuracy: {acc_cat * 100}%\")\n",
    "\n",
    "# Evaluation: sub-categories\n",
    "for sub_category in data[\"SubCategory\"].unique():\n",
    "    mask = data[\"SubCategory\"] == sub_category\n",
    "    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]\n",
    "    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)\n",
    "    print(f\"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78567eb",
   "metadata": {},
   "source": [
    "## TODO 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a83691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect words from Google Analogy dataset\n",
    "SUB_CATEGORY = \"family\"  \n",
    "\n",
    "# TODO3: Plot t-SNE for the words in the SUB_CATEGORY `: family`\n",
    "\n",
    "# 收集family子類別中的所有詞彙\n",
    "family_data = data[data[\"SubCategory\"] == SUB_CATEGORY]\n",
    "print(f\"Family子類別共有 {len(family_data)} 個類比問題\")\n",
    "\n",
    "# 從所有family類比中提取唯一的詞彙\n",
    "all_family_words = set()\n",
    "for question in family_data[\"Question\"]:\n",
    "    words = question.split()\n",
    "    for word in words:\n",
    "        all_family_words.add(word.lower())  \n",
    "\n",
    "print(f\"Family類別包含 {len(all_family_words)} 個唯一詞彙\")\n",
    "print(f\"詞彙範例: {list(all_family_words)[:10]}\")\n",
    "\n",
    "valid_words = []\n",
    "word_vectors = []\n",
    "\n",
    "for word in all_family_words:\n",
    "    if word in model.key_to_index:\n",
    "        valid_words.append(word)\n",
    "        word_vectors.append(model[word])\n",
    "\n",
    "print(f\"模型中找到 {len(valid_words)} 個詞彙\")\n",
    "word_vectors = np.array(word_vectors)\n",
    "\n",
    "# 使用t-SNE降維\n",
    "if len(valid_words) > 1:\n",
    "    print(\"正在執行t-SNE降維...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(valid_words)-1))\n",
    "    embeddings_2d = tsne.fit_transform(word_vectors)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                alpha=0.7, s=100, c='steelblue')\n",
    "    \n",
    "    # 標註每個詞彙\n",
    "    for i, word in enumerate(valid_words):\n",
    "        plt.annotate(word, \n",
    "                    (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=10, ha='left')\n",
    "    \n",
    "    plt.title(\"Word Relationships from Google Analogy Task (Family Category)\", \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"t-SNE Dimension 1\", fontsize=12)\n",
    "    plt.ylabel(\"t-SNE Dimension 2\", fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.text(0.02, 0.98, f\"Vocabulary size: {len(valid_words)}\\nModel: {MODEL_NAME}\", \n",
    "             transform=plt.gca().transAxes, fontsize=10, \n",
    "             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n=== Family類比範例 ===\")\n",
    "    for i, row in family_data.head().iterrows():\n",
    "        words = row[\"Question\"].split()\n",
    "        print(f\"{words[0]} : {words[1]} = {words[2]} : {words[3]}\")\n",
    "        \n",
    "else:\n",
    "    print(\"詞彙數量不足，無法進行t-SNE分析\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c156192",
   "metadata": {},
   "source": [
    "# Part III: Train your own word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the split Wikipedia files\n",
    "# Each file contain 562365 lines (articles).\n",
    "!gdown --id 1jiu9E1NalT2Y8EIuWNa1xf2Tw1f1XuGd -O wiki_texts_part_0.txt.gz\n",
    "!gdown --id 1ABblLRd9HXdXvaNv8H9fFq984bhnowoG -O wiki_texts_part_1.txt.gz\n",
    "!gdown --id 1z2VFNhpPvCejTP5zyejzKj5YjI_Bn42M -O wiki_texts_part_2.txt.gz\n",
    "!gdown --id 1VKjded9BxADRhIoCzXy_W8uzVOTWIf0g -O wiki_texts_part_3.txt.gz\n",
    "!gdown --id 16mBeG26m9LzHXdPe8UrijUIc6sHxhknz -O wiki_texts_part_4.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eff7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the split Wikipedia files\n",
    "# Each file contain 562365 lines (articles), except the last file.\n",
    "!gdown --id 17JFvxOH-kc-VmvGkhG7p3iSZSpsWdgJI -O wiki_texts_part_5.txt.gz\n",
    "!gdown --id 19IvB2vOJRGlrYulnTXlZECR8zT5v550P -O wiki_texts_part_6.txt.gz\n",
    "!gdown --id 1sjwO8A2SDOKruv6-8NEq7pEIuQ50ygVV -O wiki_texts_part_7.txt.gz\n",
    "!gdown --id 1s7xKWJmyk98Jbq6Fi1scrHy7fr_ellUX -O wiki_texts_part_8.txt.gz\n",
    "!gdown --id 17eQXcrvY1cfpKelLbP2BhQKrljnFNykr -O wiki_texts_part_9.txt.gz\n",
    "!gdown --id 1J5TAN6bNBiSgTIYiPwzmABvGhAF58h62 -O wiki_texts_part_10.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15058bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Extract the downloaded wiki_texts_parts files.\n",
    "!gunzip -k w# iki_texts_part_*.gz\n",
    "\n",
    "# Combine the extracted wiki_texts_parts files.\n",
    "!cat wiki_texts_part_*.txt > wiki_texts_combined.txt\n",
    "\n",
    "# Check the first ten lines of the combined file\n",
    "!head -n 10 wiki_texts_combined.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00ebb37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 檢查合併後的檔案 ===\n",
      "前10行內容:\n",
      "1: anarchism is political philosophy and movement that is against all forms of authority and seeks to a...\n",
      "2: albedo change in greenland the map shows the difference between the amount of sunlight greenland ref...\n",
      "3: or is the first letter and the first vowel letter of the latin alphabet used in the modern english a...\n",
      "4: alabama is state in the southeastern region of the united states it borders tennessee to the north g...\n",
      "5: in greek mythology achilles or achilleus was hero of the trojan war who was known as being the great...\n",
      "6: abraham lincoln february april was an american lawyer politician and statesman who served as the th ...\n",
      "7: aristotle aristotélēs bc was an ancient greek philosopher and polymath his writings cover broad rang...\n",
      "8: an american in paris is jazz influenced symphonic poem or tone poem for orchestra by american compos...\n",
      "9: the academy award for best production design recognizes achievement for art direction in film the ca...\n",
      "10: the academy awards of merit commonly known as the oscars or academy awards are awards for artistic a...\n",
      "\n",
      "=== 計算總文章數 ===\n",
      "總文章數: 5,623,655\n"
     ]
    }
   ],
   "source": [
    "# 檢查合併後的檔案\n",
    "print(\"\\n=== 檢查合併後的檔案 ===\")\n",
    "with open(\"wiki_texts_combined.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = []\n",
    "    for i, line in enumerate(f):\n",
    "        lines.append(line.strip())\n",
    "        if i >= 9:  # 只讀取前10行\n",
    "            break\n",
    "    \n",
    "print(\"前10行內容:\")\n",
    "for i, line in enumerate(lines):\n",
    "    print(f\"{i+1}: {line[:100]}...\")  # 只顯示前100個字符\n",
    "\n",
    "# 計算總行數\n",
    "print(\"\\n=== 計算總文章數 ===\")\n",
    "total_lines = 0\n",
    "with open(\"wiki_texts_combined.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        total_lines += 1\n",
    "\n",
    "print(f\"總文章數: {total_lines:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de4abb9",
   "metadata": {},
   "source": [
    "## TODO 4\n",
    "\n",
    "將合併後的文章檔案，進行5%、10%、20%抽樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you need to do sampling because the corpus is too big.\n",
    "# You can further perform analysis with a greater sampling ratio.\n",
    "\n",
    "import random\n",
    "random.seed(4321)\n",
    "\n",
    "# 多個抽樣比例\n",
    "SAMPLING_RATIOS = [0.05, 0.10, 0.20]\n",
    "wiki_txt_path = \"wiki_texts_combined.txt\"\n",
    "\n",
    "for ratio in SAMPLING_RATIOS:\n",
    "    output_path = f\"wiki_texts_sampled_{int(ratio*100)}.txt\"\n",
    "\n",
    "    print(f\"\\n=== 開始抽樣 {ratio*100:.0f}% ===\")\n",
    "\n",
    "    sampled_count = 0\n",
    "    total_processed = 0\n",
    "\n",
    "    with open(wiki_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "            for line in tqdm(f, desc=f\"抽樣 {ratio*100:.0f}%\"):\n",
    "                total_processed += 1\n",
    "                if random.random() < ratio:\n",
    "                    output_file.write(line)\n",
    "                    sampled_count += 1\n",
    "\n",
    "    actual_ratio = sampled_count / total_processed if total_processed else 0\n",
    "    print(f\"總處理文章數: {total_processed:,}\")\n",
    "    print(f\"抽樣文章數: {sampled_count:,}\")\n",
    "    print(f\"實際抽樣比例: {actual_ratio:.4f} ({actual_ratio*100:.2f}%)\")\n",
    "    print(f\"檔案已儲存為: {output_path}\")\n",
    "\n",
    "    print(f\"檔案 {output_path} 前5行:\")\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            print(f\"  {i+1}: {line.strip()[:100]}...\")\n",
    "            if i >= 4:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a69e4",
   "metadata": {},
   "source": [
    "## TODO 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6cffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "254988c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下載缺失的NLTK資源...\n",
      "下載 punkt_tab...\n",
      "✓ punkt_tab 下載成功\n",
      "✓ 分詞測試成功：['Hello', 'world', ',', 'this', 'is', 'a', 'test', '.']\n",
      "下載NLTK資源...\n",
      "資源下載完成！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/chihhung/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "print(\"下載缺失的NLTK資源...\")\n",
    "\n",
    "# 下載新版本需要的資源\n",
    "try:\n",
    "    print(\"下載 punkt_tab...\")\n",
    "    nltk.download('punkt_tab', quiet=False)\n",
    "    print(\"✓ punkt_tab 下載成功\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ punkt_tab 下載失敗: {e}\")\n",
    "\n",
    "# 重新測試分詞功能\n",
    "try:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(\"Hello world, this is a test.\")\n",
    "    print(f\"✓ 分詞測試成功：{tokens}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ 分詞測試失敗: {e}\")\n",
    "    \n",
    "    # 如果還是失敗，嘗試下載所有punkt相關資源\n",
    "    punkt_resources = ['punkt', 'punkt_tab']\n",
    "    for resource in punkt_resources:\n",
    "        try:\n",
    "            print(f\"嘗試下載 {resource}...\")\n",
    "            nltk.download(resource, quiet=False)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"下載NLTK資源...\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "print(\"資源下載完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36653e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from typing import Iterator, List\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO5: Train your own word embeddings with the sampled articles\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "# Hint: You should perform some pre-processing before training.\n",
    "\n",
    "print(\"=== 開始訓練自己的詞嵌入模型 ===\")\n",
    "\n",
    "# ---------- 讓 BLAS/NumPy 不要把 CPU 線程開爆，避免 Jupyter 當機 ----------\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "# ---------- 初始化工具 ----------\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# 添加自定義停用詞\n",
    "custom_stopwords = {'would', 'could', 'should', 'may', 'might', 'must', 'shall', 'will'}\n",
    "english_stopwords.update(custom_stopwords)\n",
    "\n",
    "# 正規表達式預編譯\n",
    "_RE_SPACES = re.compile(r'\\s+')\n",
    "_RE_NONASCII = re.compile(r'[^\\x00-\\x7F]+')  # 移除非ASCII字符\n",
    "_RE_DIGITS = re.compile(r'\\d+')\n",
    "_RE_SINGLE_CHAR = re.compile(r'\\b\\w\\b')  # 單字符詞\n",
    "\n",
    "def is_english_word(word: str) -> bool:\n",
    "    return word.isalpha() and word.encode('ascii', 'ignore').decode('ascii') == word\n",
    "\n",
    "def preprocess_text(line: str, use_lemmatization: bool = True, \n",
    "                           remove_stopwords: bool = True) -> List[str]:\n",
    "    \n",
    "    # 1. 基本清理\n",
    "    text = line.lower().strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # 移除非ASCII字符\n",
    "    text = _RE_NONASCII.sub(' ', text)\n",
    "    # 移除數字\n",
    "    text = _RE_DIGITS.sub(' ', text)\n",
    "    # 移除單字符詞\n",
    "    text = _RE_SINGLE_CHAR.sub(' ', text)\n",
    "    # 正規化空白字符\n",
    "    text = _RE_SPACES.sub(' ', text).strip()\n",
    "    \n",
    "    # 2. 改進的分詞（Better tokenization）\n",
    "    try:\n",
    "        tokens = word_tokenize(text)  \n",
    "    except:\n",
    "        tokens = text.split()  \n",
    "    \n",
    "    processed_words = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # 移除標點符號\n",
    "        token = re.sub(r'[^\\w]', '', token)\n",
    "        if not token or len(token) < 2:\n",
    "            continue\n",
    "            \n",
    "        # 3. 檢查是否為英文詞彙（Remove non-English words）\n",
    "        if not is_english_word(token):\n",
    "            continue\n",
    "            \n",
    "        # 4. 移除停用詞（Remove stop words）\n",
    "        if remove_stopwords and token in english_stopwords:\n",
    "            continue\n",
    "            \n",
    "        # 5. 詞元化（Lemmatization）\n",
    "        if use_lemmatization:\n",
    "            try:\n",
    "                token = lemmatizer.lemmatize(token, pos='v')  # 動詞詞元化\n",
    "                token = lemmatizer.lemmatize(token, pos='n')  # 名詞詞元化\n",
    "            except:\n",
    "                pass  # 如果詞元化失敗，保持原詞\n",
    "        \n",
    "        processed_words.append(token)\n",
    "    \n",
    "    return processed_words\n",
    "\n",
    "class LineSentencePreprocessed:\n",
    "\n",
    "    def __init__(self, path: str, min_len: int = 5, show_progress: bool = True,\n",
    "                 use_lemmatization: bool = True, remove_stopwords: bool = True):\n",
    "        self.path = path\n",
    "        self.min_len = min_len\n",
    "        self.show_progress = show_progress\n",
    "        self.use_lemmatization = use_lemmatization\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        \n",
    "        try:\n",
    "            with open(self.path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                self.total_lines = sum(1 for _ in f)\n",
    "        except Exception:\n",
    "            self.total_lines = None\n",
    "\n",
    "    def __iter__(self) -> Iterator[List[str]]:\n",
    "        with open(self.path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            iterator = f\n",
    "            if self.show_progress:\n",
    "                iterator = tqdm(f, total=self.total_lines, desc=\"進階預處理語料\", leave=False)\n",
    "            \n",
    "            for line in iterator:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                    \n",
    "                words = preprocess_text(\n",
    "                    line, \n",
    "                    use_lemmatization=self.use_lemmatization,\n",
    "                    remove_stopwords=self.remove_stopwords\n",
    "                )\n",
    "                \n",
    "                if len(words) >= self.min_len:\n",
    "                    yield words\n",
    "\n",
    "\n",
    "class TqdmEpochLogger(CallbackAny2Vec):\n",
    "    def __init__(self, total_epochs: int):\n",
    "        self.total_epochs = total_epochs\n",
    "        self.epoch = 0\n",
    "        self.prev_cum_loss = 0.0\n",
    "        self.pbar = tqdm(total=total_epochs, desc=\"訓練 Epoch\", position=0)\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        cum_loss = model.get_latest_training_loss()\n",
    "        delta = cum_loss - self.prev_cum_loss\n",
    "        self.prev_cum_loss = cum_loss\n",
    "        self.epoch += 1\n",
    "        self.pbar.set_postfix(loss_delta=f\"{delta:.2f}\", cum_loss=f\"{cum_loss:.2f}\")\n",
    "        self.pbar.update(1)\n",
    "        if self.epoch >= self.total_epochs:\n",
    "            self.pbar.close()\n",
    "\n",
    "\n",
    "# 訓練設置\n",
    "corpus_path = \"wiki_texts_sampled_20.txt\"\n",
    "epochs = 10\n",
    "\n",
    "# 比較不同預處理策略\n",
    "preprocessing_configs = [\n",
    "    {\n",
    "        'name': '預處理版本1',\n",
    "        'use_lemmatization': True,\n",
    "        'remove_stopwords': True,\n",
    "        'min_count': 3  # 因為移除了停用詞，可以降低最小詞頻\n",
    "    }\n",
    "]\n",
    "\n",
    "models_results = {}\n",
    "\n",
    "for config in preprocessing_configs:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"訓練模型：{config['name']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    word2vec_params = {\n",
    "        \"vector_size\": 300,\n",
    "        \"window\": 5,\n",
    "        \"min_count\": config['min_count'],\n",
    "        \"workers\": max(1, min(4, mp.cpu_count() // 2)),\n",
    "        \"sg\": 0,  # CBOW\n",
    "        \"negative\": 5,\n",
    "        \"sample\": 1e-3,\n",
    "        \"seed\": 4321,\n",
    "    }\n",
    "    \n",
    "    print(\"訓練參數：\")\n",
    "    for k, v in word2vec_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(f\"  epochs: {epochs}\")\n",
    "    print(f\"  詞元化: {config['use_lemmatization']}\")\n",
    "    print(f\"  移除停用詞: {config['remove_stopwords']}\")\n",
    "    \n",
    "    # 建立語料迭代器\n",
    "    sentences = LineSentencePreprocessed(\n",
    "        corpus_path, \n",
    "        min_len=5,\n",
    "        show_progress=True,\n",
    "        use_lemmatization=config['use_lemmatization'],\n",
    "        remove_stopwords=config['remove_stopwords']\n",
    "    )\n",
    "    \n",
    "    # 建立並訓練模型\n",
    "    model = Word2Vec(**word2vec_params)\n",
    "    print(\"\\n=== 建立詞彙表 ===\")\n",
    "    model.build_vocab(sentences, progress_per=10000, keep_raw_vocab=False)\n",
    "    print(f\"詞彙表大小：{len(model.wv):,}\")\n",
    "    \n",
    "    print(\"\\n=== 開始訓練 ===\")\n",
    "    epoch_logger = TqdmEpochLogger(total_epochs=epochs)\n",
    "    \n",
    "    # 重新創建語料迭代器用於訓練\n",
    "    training_sentences = LineSentencePreprocessed(\n",
    "        corpus_path, \n",
    "        min_len=5,\n",
    "        show_progress=False,\n",
    "        use_lemmatization=config['use_lemmatization'],\n",
    "        remove_stopwords=config['remove_stopwords']\n",
    "    )\n",
    "    \n",
    "    model.train(\n",
    "        corpus_iterable=training_sentences,\n",
    "        total_examples=model.corpus_count,\n",
    "        epochs=epochs,\n",
    "        compute_loss=True,\n",
    "        callbacks=[epoch_logger]\n",
    "    )\n",
    "    \n",
    "    # 測試模型\n",
    "    test_words = [\"king\", \"queen\", \"man\", \"woman\", \"computer\", \"science\", \"rock\", \"stone\"]\n",
    "    in_vocab = [w for w in test_words if w in model.wv]\n",
    "    \n",
    "    print(f\"\\n=== 模型測試：{config['name']} ===\")\n",
    "    print(f\"測試詞彙（存在於詞彙表）：{in_vocab}\")\n",
    "    \n",
    "    if in_vocab:\n",
    "        for w in in_vocab[:3]:\n",
    "            print(f\"\\n與 '{w}' 最相似的詞：\")\n",
    "            try:\n",
    "                for sw, sim in model.wv.most_similar(w, topn=5):\n",
    "                    print(f\"  {sw:20s} {sim:.3f}\")\n",
    "            except KeyError:\n",
    "                print(f\"  詞彙 '{w}' 不在模型中\")\n",
    "    \n",
    "    # 保存模型\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    model_name = config['name'].replace('（', '_').replace('）', '').replace(' ', '_').replace('、', '_')\n",
    "    model_save_path = f\"./models/word2vec_{model_name}.model\"\n",
    "    vectors_save_path = f\"./models/word_vectors_{model_name}.kv\"\n",
    "    \n",
    "    model.save(model_save_path)\n",
    "    model.wv.save(vectors_save_path)\n",
    "    \n",
    "    print(f\"\\n模型已保存：{model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ade1e9",
   "metadata": {},
   "source": [
    "# Part IV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305d511",
   "metadata": {},
   "source": [
    "## TODO 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"questions-words.csv\")\n",
    "\n",
    "# Do predictions and preserve the gold answers (word_D)\n",
    "print(\"=== 載入自訓練的詞嵌入模型 ===\")\n",
    "\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    my_model = Word2Vec.load(\"./models/word2vec_改進預處理_含詞元化和停用詞移除.model\")\n",
    "    print(\"✓ 成功載入完整模型\")\n",
    "except:\n",
    "    try:\n",
    "        from gensim.models import KeyedVectors\n",
    "        my_model = KeyedVectors.load(\"./models/word_vectors_改進預處理_含詞元化和停用詞移除.kv\")\n",
    "        print(\"✓ 成功載入詞向量\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ 載入模型失敗: {e}\")\n",
    "        print(\"請確認模型檔案路徑正確\")\n",
    "        raise\n",
    "\n",
    "print(f\"模型詞彙表大小: {len(my_model.wv if hasattr(my_model, 'wv') else my_model):,}\")\n",
    "\n",
    "word_vectors = my_model.wv if hasattr(my_model, 'wv') else my_model\n",
    "\n",
    "preds = []\n",
    "golds = []\n",
    "stats = {\n",
    "    'total': 0,\n",
    "    'oov_words': 0,  # 詞彙表外的詞\n",
    "    'valid_predictions': 0,\n",
    "    'oov_analogies': []  # 記錄有OOV詞的類比\n",
    "}\n",
    "\n",
    "print(\"\\n=== 開始使用自訓練模型進行類比預測 ===\")\n",
    "\n",
    "\n",
    "for analogy in tqdm(data[\"Question\"], desc=\"處理類比\"):\n",
    "      # TODO6: Write your code here to use your trained word embeddings for getting predictions of the analogy task.\n",
    "      # You should also preserve the gold answers during iterations for evaluations later.\n",
    "      \"\"\" Hints\n",
    "      # Unpack the analogy (e.g., \"man\", \"woman\", \"king\", \"queen\")\n",
    "      # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d\n",
    "      # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776\n",
    "      # Mikolov et al., 2013: big - biggest and small - smallest\n",
    "      # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).\n",
    "      \"\"\"\n",
    "      \n",
    "      words = analogy.split()\n",
    "      word_a, word_b, word_c, word_d = words\n",
    "      \n",
    "      # 轉換為小寫\n",
    "      word_a_lower = word_a.lower()\n",
    "      word_b_lower = word_b.lower()\n",
    "      word_c_lower = word_c.lower()\n",
    "      word_d_lower = word_d.lower()\n",
    "      \n",
    "      # 保存正確答案\n",
    "      golds.append(word_d_lower)\n",
    "      stats['total'] += 1\n",
    "      \n",
    "      # 檢查所有詞是否都在模型的詞彙表中\n",
    "      if all(word in word_vectors.key_to_index for word in [word_a_lower, word_b_lower, word_c_lower]):\n",
    "            \n",
    "            # 執行向量運算: word_b - word_a + word_c ≈ word_d\n",
    "            result = word_vectors.most_similar(\n",
    "            positive=[word_b_lower, word_c_lower], \n",
    "            negative=[word_a_lower],               \n",
    "            topn=10                                \n",
    "            )\n",
    "            \n",
    "            # 找到第一個不是輸入詞的預測結果\n",
    "            prediction = None\n",
    "            for candidate, similarity in result:\n",
    "                if candidate not in [word_a_lower, word_b_lower, word_c_lower]:\n",
    "                    prediction = candidate\n",
    "                    break\n",
    "            \n",
    "            # 如果找到預測結果，使用它；否則使用最相似的詞\n",
    "            if prediction is None:\n",
    "                prediction = result[0][0]\n",
    "            \n",
    "            preds.append(prediction)\n",
    "            stats['valid_predictions'] += 1\n",
    "            \n",
    "      else:\n",
    "            # 檢查哪些詞不在詞彙表中\n",
    "            missing_words = [word for word in [word_a_lower, word_b_lower, word_c_lower] \n",
    "                        if word not in word_vectors.key_to_index]\n",
    "            stats['oov_words'] += len(missing_words)\n",
    "            stats['oov_analogies'].append({\n",
    "            'analogy': analogy,\n",
    "            'missing_words': missing_words\n",
    "            })\n",
    "            \n",
    "            # 對於詞彙表外的詞，預測為未知\n",
    "            preds.append(\"<OOV>\")\n",
    "            \n",
    "\n",
    "      print(f\"\\n=== 預測完成統計 ===\")\n",
    "      print(f\"總類比數量: {stats['total']:,}\")\n",
    "      print(f\"有效預測: {stats['valid_predictions']:,} ({stats['valid_predictions']/stats['total']*100:.1f}%)\")\n",
    "      print(f\"詞彙表外詞數: {stats['oov_words']:,}\")\n",
    "      print(f\"包含OOV的類比: {len(stats['oov_analogies']):,}\")\n",
    "\n",
    "      print(f\"\\n預測範例: {preds[:10]}\")\n",
    "      print(f\"正確答案範例: {golds[:10]}\")\n",
    "\n",
    "      # 分析詞彙覆蓋情況\n",
    "      if stats['oov_analogies']:\n",
    "        print(f\"\\n=== 詞彙表外詞分析（前10個） ===\")\n",
    "        for i, oov_info in enumerate(stats['oov_analogies'][:10]):\n",
    "                print(f\"{i+1}. {oov_info['analogy']} - 缺失: {oov_info['missing_words']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f3fa56",
   "metadata": {},
   "source": [
    "## TODO 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e51a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect words from Google Analogy dataset\n",
    "SUB_CATEGORY = \"family\" \n",
    "\n",
    "# TODO7: Plot t-SNE for the words in the SUB_CATEGORY `family`\n",
    "\n",
    "print(f\"=== 使用自訓練模型視覺化 {SUB_CATEGORY} 類別詞彙關係 ===\")\n",
    "\n",
    "try:\n",
    "    # 使用之前載入的自訓練模型\n",
    "    word_vectors = my_model.wv if hasattr(my_model, 'wv') else my_model\n",
    "    print(f\"✓ 使用自訓練模型，詞彙表大小: {len(word_vectors):,}\")\n",
    "except:\n",
    "    print(\"✗ 自訓練模型不可用，請先載入模型\")\n",
    "    raise\n",
    "\n",
    "# 提取family子類別的數據\n",
    "family_data = data[data[\"SubCategory\"] == SUB_CATEGORY]\n",
    "print(f\"Family子類別共有 {len(family_data)} 個類比問題\")\n",
    "\n",
    "\n",
    "if len(family_data) == 0:\n",
    "    print(\"⚠️ 未找到family類別資料，檢查可用的子類別:\")\n",
    "    print(data[\"SubCategory\"].unique())\n",
    "else:\n",
    "    all_family_words = set()\n",
    "    for question in family_data[\"Question\"]:\n",
    "        words = question.split()\n",
    "        for word in words:\n",
    "            all_family_words.add(word.lower()) \n",
    "\n",
    "    print(f\"Family類別包含 {len(all_family_words)} 個唯一詞彙\")\n",
    "    print(f\"詞彙範例: {list(all_family_words)[:10]}\")\n",
    "\n",
    "    # 檢查哪些詞在自訓練模型中\n",
    "    valid_words = []\n",
    "    word_vectors_list = []\n",
    "    missing_words = []\n",
    "\n",
    "    for word in all_family_words:\n",
    "        if word in word_vectors.key_to_index:\n",
    "            valid_words.append(word)\n",
    "            word_vectors_list.append(word_vectors[word])\n",
    "        else:\n",
    "            missing_words.append(word)\n",
    "\n",
    "    print(f\"自訓練模型中找到 {len(valid_words)} 個詞彙\")\n",
    "    if missing_words:\n",
    "        print(f\"模型中缺失的詞彙: {missing_words}\")\n",
    "\n",
    "    # 進行t-SNE降維和視覺化\n",
    "    if len(valid_words) > 1:\n",
    "        print(\"正在執行t-SNE降維...\")\n",
    "        \n",
    "        # 準備數據\n",
    "        word_vectors_array = np.array(word_vectors_list)\n",
    "        \n",
    "        # 設置t-SNE參數\n",
    "        perplexity = min(30, len(valid_words) - 1)\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, \n",
    "                   n_iter=1000, learning_rate=200)\n",
    "        \n",
    "        # 執行t-SNE\n",
    "        embeddings_2d = tsne.fit_transform(word_vectors_array)\n",
    "        \n",
    "        # 創建視覺化\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # 繪製散點圖\n",
    "        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                            alpha=0.7, s=120, c='steelblue', edgecolors='darkblue')\n",
    "        \n",
    "        # 標註每個詞彙\n",
    "        for i, word in enumerate(valid_words):\n",
    "            plt.annotate(word, \n",
    "                        (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=11, ha='left', va='bottom',\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "        \n",
    "        plt.title(\"Word Relationships from Google Analogy Task\\n(Family Category - Self-trained Model)\", \n",
    "                  fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.xlabel(\"t-SNE Dimension 1\", fontsize=12)\n",
    "        plt.ylabel(\"t-SNE Dimension 2\", fontsize=12)\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        info_text = f\"Vocabulary Size: {len(valid_words)}\\nSelf-trained Model\\nVector Dimension: {word_vectors.vector_size}\"\n",
    "        plt.text(0.02, 0.98, info_text, \n",
    "                transform=plt.gca().transAxes, fontsize=10, \n",
    "                verticalalignment='top', \n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n=== Family類比範例 ===\")\n",
    "        for i, (_, row) in enumerate(family_data.head().iterrows()):\n",
    "            words = row[\"Question\"].split()\n",
    "            print(f\"{i+1}. {words[0]} : {words[1]} = {words[2]} : {words[3]}\")\n",
    "            \n",
    "        # 分析詞彙間的相似性\n",
    "        print(f\"\\n=== 詞彙相似性分析 ===\")\n",
    "        if len(valid_words) >= 4:\n",
    "            sample_words = valid_words[:4]\n",
    "            print(\"部分詞彙間的餘弦相似度:\")\n",
    "            for i, word1 in enumerate(sample_words):\n",
    "                for word2 in sample_words[i+1:]:\n",
    "                    try:\n",
    "                        similarity = word_vectors.similarity(word1, word2)\n",
    "                        print(f\"  {word1} ↔ {word2}: {similarity:.3f}\")\n",
    "                    except:\n",
    "                        print(f\"  {word1} ↔ {word2}: 無法計算\")\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️ 詞彙數量不足，無法進行t-SNE分析\")\n",
    "\n",
    "print(f\"\\n=== 分析完成 ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c3b93f",
   "metadata": {},
   "source": [
    "## 比較不同的語料庫訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef57a90",
   "metadata": {},
   "source": [
    "### 先下載不同的語料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "from nltk.corpus import brown, reuters\n",
    "\n",
    "def download_public_corpora():\n",
    "    print(\"正在下載公開語料庫...\")\n",
    "    \n",
    "    # 下載NLTK語料庫\n",
    "    nltk.download('brown', quiet=True)\n",
    "    nltk.download('reuters', quiet=True)\n",
    "    \n",
    "    # 創建新聞語料庫（使用Reuters）\n",
    "    from nltk.corpus import reuters\n",
    "    with open('news_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "        for fileid in reuters.fileids()[:1000]:  # 取前1000篇文章\n",
    "            text = reuters.raw(fileid).lower()\n",
    "            # 簡單清理\n",
    "            lines = text.split('\\n')\n",
    "            for line in lines:\n",
    "                if len(line.split()) > 5:  # 只保留足夠長的句子\n",
    "                    f.write(line.strip() + '\\n')\n",
    "    \n",
    "    print(\"✓ news_corpus.txt 創建完成\")\n",
    "    \n",
    "    # 創建文學語料庫（使用Brown的文學部分）\n",
    "    from nltk.corpus import brown\n",
    "    with open('literature_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "        # Brown語料庫的文學類別\n",
    "        literature_categories = ['romance', 'mystery', 'science_fiction', 'adventure']\n",
    "        for category in literature_categories:\n",
    "            if category in brown.categories():\n",
    "                for sent in brown.sents(categories=category):\n",
    "                    sentence = ' '.join(sent).lower()\n",
    "                    if len(sentence.split()) > 5:\n",
    "                        f.write(sentence + '\\n')\n",
    "    \n",
    "    print(\"✓ literature_corpus.txt 創建完成\")\n",
    "\n",
    "    \n",
    "    return True\n",
    "\n",
    "success = download_public_corpora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acf56c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "設定加速參數:\n",
      "   最大語料庫大小: 80,000 句\n",
      "   評估樣本大小: 3,000 問題\n",
      "開始綜合評估\n",
      "最大語料庫大小: 80,000 句\n",
      "評估樣本大小: 3,000 問題\n",
      "\n",
      "============================================================\n",
      "階段 1: 訓練模型\n",
      "============================================================\n",
      "=== 開始在不同語料庫上訓練模型 ===\n",
      "加速設定: epochs=5, 最大語料80,000句\n",
      "\n",
      "[1/3] 訓練模型：維基百科20%抽樣\n",
      "============================================================\n",
      "載入語料庫: wiki_texts_sampled_20.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預處理語料:   1%|▏         | 80000/5623655 [00:43<49:45, 1857.04it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "達到最大行數限制: 80,000\n",
      "載入完成: 80,000 句\n",
      "建立Word2Vec模型...\n",
      "維基百科20%抽樣 訓練完成!\n",
      "   訓練時間: 311.0 秒\n",
      "   語料大小: 80,000 句\n",
      "\n",
      "[2/3] 訓練模型：新聞語料庫\n",
      "============================================================\n",
      "載入語料庫: news_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預處理語料:   0%|          | 12597/5623655 [00:00<01:08, 81561.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "載入完成: 11,757 句\n",
      "建立Word2Vec模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新聞語料庫 訓練完成!\n",
      "   訓練時間: 1.0 秒\n",
      "   語料大小: 11,757 句\n",
      "\n",
      "[3/3] 訓練模型：文學作品語料庫\n",
      "============================================================\n",
      "載入語料庫: literature_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "預處理語料:   0%|          | 12457/5623655 [00:02<18:44, 4991.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "載入完成: 11,542 句\n",
      "建立Word2Vec模型...\n",
      "文學作品語料庫 訓練完成!\n",
      "   訓練時間: 2.9 秒\n",
      "   語料大小: 11,542 句\n",
      "\n",
      "============================================================\n",
      "階段 2: 評估模型\n",
      "============================================================\n",
      "\n",
      "[1/3] 評估模型: wikipedia_20\n",
      "\n",
      "評估模型：wikipedia_20\n",
      "使用抽樣評估加速 (樣本大小: 3,000)\n",
      "抽樣 3,000/19,544 個問題進行評估\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "評估類別: 100%|██████████| 2/2 [00:28<00:00, 14.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評估完成!\n",
      "   評估時間: 28.7 秒\n",
      "   準確率: 0.5880\n",
      "   覆蓋率: 0.9977\n",
      "   正確: 1,764\n",
      "   OOV: 7\n",
      "\n",
      "[2/3] 評估模型: news_corpus\n",
      "\n",
      "評估模型：news_corpus\n",
      "使用抽樣評估加速 (樣本大小: 3,000)\n",
      "抽樣 3,000/19,544 個問題進行評估\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "評估類別: 100%|██████████| 2/2 [00:00<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評估完成!\n",
      "   評估時間: 0.1 秒\n",
      "   準確率: 0.0000\n",
      "   覆蓋率: 0.0357\n",
      "   正確: 0\n",
      "   OOV: 2,893\n",
      "\n",
      "[3/3] 評估模型: literature_corpus\n",
      "\n",
      "評估模型：literature_corpus\n",
      "使用抽樣評估加速 (樣本大小: 3,000)\n",
      "抽樣 3,000/19,544 個問題進行評估\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "評估類別: 100%|██████████| 2/2 [00:00<00:00, 11.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評估完成!\n",
      "   評估時間: 0.2 秒\n",
      "   準確率: 0.0003\n",
      "   覆蓋率: 0.1113\n",
      "   正確: 1\n",
      "   OOV: 2,666\n",
      "\n",
      "============================================================\n",
      "階段 3: 生成報告\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "多語料庫比較報告\n",
      "================================================================================\n",
      "\n",
      "整體性能比較：\n",
      "模型名稱                 準確率        詞彙表大小        覆蓋率        訓練時間      \n",
      "---------------------------------------------------------------------------\n",
      "wikipedia_20         0.5880     0.9977     311.0     s\n",
      "news_corpus          0.0000     0.0357     1.0       s\n",
      "literature_corpus    0.0003     0.1113     2.9       s\n",
      "\n",
      "按主類別性能比較：\n",
      "\n",
      "semantic 類別：\n",
      "模型                   準確率        正確數/總數         \n",
      "--------------------------------------------------\n",
      "wikipedia_20         0.6224     839/1348\n",
      "news_corpus          0.0000     0/13\n",
      "literature_corpus    0.0000     0/27\n",
      "\n",
      "syntactic 類別：\n",
      "模型                   準確率        正確數/總數         \n",
      "--------------------------------------------------\n",
      "wikipedia_20         0.5623     925/1645\n",
      "literature_corpus    0.0033     1/307\n",
      "news_corpus          0.0000     0/94\n",
      "\n",
      "最佳表現分析：\n",
      "最高準確率: wikipedia_20 (0.5880)\n",
      "最高覆蓋率: wikipedia_20 (0.9977)\n",
      "最快訓練: news_corpus (1.0s)\n",
      "\n",
      "報告生成完成!\n",
      "\n",
      "綜合評估完成!\n",
      "總耗時: 344.0 秒 (5.7 分鐘)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import time\n",
    "\n",
    "# 預處理相關函數\n",
    "_RE_SPACES = re.compile(r'\\s+')\n",
    "_RE_NONWS = re.compile(r'[^\\w\\s]')\n",
    "_RE_DIGITS = re.compile(r'\\d+')\n",
    "\n",
    "def preprocess_text(line: str):\n",
    "    t = line.lower()\n",
    "    t = _RE_SPACES.sub(' ', t)\n",
    "    t = _RE_NONWS.sub(' ', t)\n",
    "    t = _RE_DIGITS.sub('', t)  \n",
    "    words = [w for w in t.split() if len(w) >= 2]\n",
    "    return words\n",
    "\n",
    "class FastLineSentencePreprocessed:\n",
    "    def __init__(self, path: str, min_len: int = 5, max_lines: int = None):\n",
    "        self.path = path\n",
    "        self.min_len = min_len\n",
    "        self.max_lines = max_lines\n",
    "        self.sentences = self._load_sentences()\n",
    "    \n",
    "    def _load_sentences(self):\n",
    "        print(f\"載入語料庫: {self.path}\")\n",
    "        sentences = []      \n",
    "        \n",
    "        with open(self.path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            iterator = f\n",
    "            if total_lines:\n",
    "                iterator = tqdm(f, total=total_lines, desc=\"預處理語料\", leave=True)\n",
    "                \n",
    "            for i, line in enumerate(iterator):\n",
    "                if self.max_lines and i >= self.max_lines:\n",
    "                    print(f\"達到最大行數限制: {self.max_lines:,}\")\n",
    "                    break\n",
    "                    \n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    words = preprocess_text(line)\n",
    "                    if len(words) >= self.min_len:\n",
    "                        sentences.append(words)\n",
    "        \n",
    "        print(f\"載入完成: {len(sentences):,} 句\")\n",
    "        return sentences\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "class MultiCorpusEvaluator:\n",
    "    def __init__(self, analogy_data_path: str, max_corpus_size: int = 80000, eval_sample_size: int = 3000):\n",
    "        self.analogy_data = pd.read_csv(analogy_data_path)\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.max_corpus_size = max_corpus_size  \n",
    "        self.eval_sample_size = eval_sample_size \n",
    "        print(f\"設定加速參數:\")\n",
    "        print(f\"   最大語料庫大小: {max_corpus_size:,} 句\")\n",
    "        print(f\"   評估樣本大小: {eval_sample_size:,} 問題\")\n",
    "        \n",
    "    def prepare_corpus_configs(self):\n",
    "        corpus_configs = {\n",
    "            'wikipedia_20': {\n",
    "                'path': 'wiki_texts_sampled_20.txt',\n",
    "                'description': '維基百科20%抽樣',\n",
    "                'domain': '百科全書',\n",
    "                'style': '正式學術'\n",
    "            },\n",
    "            'news_corpus': {\n",
    "                'path': 'news_corpus.txt',\n",
    "                'description': '新聞語料庫',\n",
    "                'domain': '新聞時事',\n",
    "                'style': '新聞體'\n",
    "            },\n",
    "            'literature_corpus': {\n",
    "                'path': 'literature_corpus.txt', \n",
    "                'description': '文學作品語料庫',\n",
    "                'domain': '文學創作',\n",
    "                'style': '文學性'\n",
    "            }\n",
    "        }\n",
    "        return corpus_configs\n",
    "    \n",
    "    def train_models_on_different_corpora(self):\n",
    "        corpus_configs = self.prepare_corpus_configs()\n",
    "\n",
    "        word2vec_params = {\n",
    "            \"vector_size\": 300,\n",
    "            \"window\": 5,\n",
    "            \"min_count\": 5,\n",
    "            \"workers\": 4,\n",
    "            \"sg\": 0,  \n",
    "            \"epochs\": 5, \n",
    "            \"negative\": 5,\n",
    "            \"sample\": 1e-3,\n",
    "            \"seed\": 4321,\n",
    "        }\n",
    "        \n",
    "        print(\"=== 開始在不同語料庫上訓練模型 ===\")\n",
    "        print(f\"加速設定: epochs={word2vec_params['epochs']}, 最大語料{self.max_corpus_size:,}句\")\n",
    "        \n",
    "        total_corpora = len([c for c in corpus_configs.values() if os.path.exists(c['path'])])\n",
    "        current_corpus = 0\n",
    "        \n",
    "        for corpus_name, config in corpus_configs.items():\n",
    "            if not os.path.exists(config['path']):\n",
    "                print(f\"跳過 {corpus_name}：檔案 {config['path']} 不存在\")\n",
    "                continue\n",
    "            \n",
    "            current_corpus += 1\n",
    "            print(f\"\\n[{current_corpus}/{total_corpora}] 訓練模型：{config['description']}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "\n",
    "            sentences = FastLineSentencePreprocessed(\n",
    "                config['path'], \n",
    "                min_len=5,\n",
    "                max_lines=self.max_corpus_size  \n",
    "            )\n",
    "            \n",
    "            # 建立模型\n",
    "            print(\"建立Word2Vec模型...\")\n",
    "            model = Word2Vec(\n",
    "                sentences=sentences,  \n",
    "                **word2vec_params\n",
    "            )\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            self.models[corpus_name] = {\n",
    "                'model': model.wv,\n",
    "                'config': config,\n",
    "                'training_time': training_time,\n",
    "                'corpus_size': len(sentences)\n",
    "            }\n",
    "            \n",
    "            print(f\"{config['description']} 訓練完成!\")\n",
    "            print(f\"   訓練時間: {training_time:.1f} 秒\")\n",
    "            print(f\"   語料大小: {len(sentences):,} 句\")\n",
    "            \n",
    "\n",
    "    def evaluate_model_on_analogies(self, model_name: str, model_wv) -> Dict:        \n",
    "        print(f\"\\n評估模型：{model_name}\")\n",
    "        print(f\"使用抽樣評估加速 (樣本大小: {self.eval_sample_size:,})\")\n",
    "        \n",
    "        total_data = len(self.analogy_data)\n",
    "        if total_data > self.eval_sample_size:\n",
    "            eval_data = self.analogy_data.sample(n=self.eval_sample_size, random_state=42)\n",
    "            print(f\"抽樣 {self.eval_sample_size:,}/{total_data:,} 個問題進行評估\")\n",
    "        else:\n",
    "            eval_data = self.analogy_data\n",
    "            print(f\"評估全部 {total_data:,} 個問題\")\n",
    "            \n",
    "        results = {\n",
    "            'total': len(eval_data),\n",
    "            'correct': 0,\n",
    "            'oov_count': 0,\n",
    "            'category_results': {},\n",
    "            'subcategory_results': {}\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        categories = eval_data['Category'].unique()\n",
    "        \n",
    "        for category in tqdm(categories, desc=\"評估類別\"):\n",
    "            category_data = eval_data[eval_data['Category'] == category]\n",
    "            category_correct = 0\n",
    "            category_valid = 0\n",
    "            \n",
    "            for _, row in tqdm(category_data.iterrows(), \n",
    "                             total=len(category_data), \n",
    "                             desc=f\"  {category}\",\n",
    "                             leave=False):\n",
    "                \n",
    "                words = row['Question'].split()\n",
    "                word_a, word_b, word_c, word_d = [w.lower() for w in words]\n",
    "                \n",
    "                if all(w in model_wv.key_to_index for w in [word_a, word_b, word_c]):\n",
    "                    similar_words = model_wv.most_similar(\n",
    "                        positive=[word_b, word_c],\n",
    "                        negative=[word_a],\n",
    "                        topn=5  \n",
    "                    )\n",
    "                    \n",
    "                    prediction = None\n",
    "                    for candidate, _ in similar_words:\n",
    "                        if candidate not in [word_a, word_b, word_c]:\n",
    "                            prediction = candidate\n",
    "                            break\n",
    "                    \n",
    "                    if prediction == word_d.lower():\n",
    "                        results['correct'] += 1\n",
    "                        category_correct += 1\n",
    "                    \n",
    "                    category_valid += 1\n",
    "                else:\n",
    "                    results['oov_count'] += 1\n",
    "            \n",
    "            if category_valid > 0:\n",
    "                results['category_results'][category] = {\n",
    "                    'accuracy': category_correct / category_valid,\n",
    "                    'correct': category_correct,\n",
    "                    'total': category_valid\n",
    "                }\n",
    "        \n",
    "        eval_time = time.time() - start_time\n",
    "        accuracy = results['correct'] / results['total'] if results['total'] > 0 else 0\n",
    "        coverage = (results['total'] - results['oov_count']) / results['total']\n",
    "        \n",
    "        print(f\"評估完成!\")\n",
    "        print(f\"   評估時間: {eval_time:.1f} 秒\") \n",
    "        print(f\"   準確率: {accuracy:.4f}\")\n",
    "        print(f\"   覆蓋率: {coverage:.4f}\")\n",
    "        print(f\"   正確: {results['correct']:,}\")\n",
    "        print(f\"   OOV: {results['oov_count']:,}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_comprehensive_evaluation(self):\n",
    "        total_start_time = time.time()\n",
    "        print(f\"開始綜合評估\")\n",
    "        print(f\"最大語料庫大小: {self.max_corpus_size:,} 句\")\n",
    "        print(f\"評估樣本大小: {self.eval_sample_size:,} 問題\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"階段 1: 訓練模型\")\n",
    "        print(f\"{'='*60}\")\n",
    "        self.train_models_on_different_corpora()\n",
    "        \n",
    "        # 評估所有模型\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"階段 2: 評估模型\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        total_models = len(self.models)\n",
    "        current_model = 0\n",
    "        \n",
    "        for model_name, model_info in self.models.items():\n",
    "            current_model += 1\n",
    "            print(f\"\\n[{current_model}/{total_models}] 評估模型: {model_name}\")\n",
    "            \n",
    "            results = self.evaluate_model_on_analogies(\n",
    "                model_name, \n",
    "                model_info['model']\n",
    "            )\n",
    "            self.results[model_name] = results\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"階段 3: 生成報告\")\n",
    "        print(f\"{'='*60}\")\n",
    "        self.generate_comparison_report()\n",
    "        \n",
    "        total_time = time.time() - total_start_time\n",
    "        print(f\"\\n綜合評估完成!\")\n",
    "        print(f\"總耗時: {total_time:.1f} 秒 ({total_time/60:.1f} 分鐘)\")\n",
    "    \n",
    "    def generate_comparison_report(self):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"多語料庫比較報告\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if not self.results:\n",
    "            print(\"沒有評估結果可以比較\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n整體性能比較：\")\n",
    "        print(f\"{'模型名稱':<20} {'準確率':<10} {'詞彙表大小':<12} {'覆蓋率':<10} {'訓練時間':<10}\")\n",
    "        print(\"-\" * 75)\n",
    "        \n",
    "        model_performances = []\n",
    "        \n",
    "        for model_name, results in self.results.items():\n",
    "            accuracy = results['correct'] / results['total'] if results['total'] > 0 else 0\n",
    "            coverage = (results['total'] - results['oov_count']) / results['total'] if results['total'] > 0 else 0\n",
    "            training_time = self.models[model_name].get('training_time', 0)\n",
    "            \n",
    "            model_performances.append({\n",
    "                'name': model_name,\n",
    "                'accuracy': accuracy,\n",
    "                'coverage': coverage,\n",
    "                'training_time': training_time\n",
    "            })\n",
    "\n",
    "            print(f\"{model_name:<20} {accuracy:<10.4f} {coverage:<10.4f} {training_time:<10.1f}s\")\n",
    "\n",
    "        # 按類別比較\n",
    "        print(f\"\\n按主類別性能比較：\")\n",
    "        categories = set()\n",
    "        for results in self.results.values():\n",
    "            categories.update(results['category_results'].keys())\n",
    "        \n",
    "        for category in sorted(categories):\n",
    "            print(f\"\\n{category} 類別：\")\n",
    "            print(f\"{'模型':<20} {'準確率':<10} {'正確數/總數':<15}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            category_performances = []\n",
    "            for model_name, results in self.results.items():\n",
    "                if category in results['category_results']:\n",
    "                    cat_result = results['category_results'][category]\n",
    "                    category_performances.append({\n",
    "                        'model': model_name,\n",
    "                        'accuracy': cat_result['accuracy'],\n",
    "                        'correct': cat_result['correct'],\n",
    "                        'total': cat_result['total']\n",
    "                    })\n",
    "            \n",
    "            category_performances.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "            for perf in category_performances:\n",
    "                print(f\"{perf['model']:<20} {perf['accuracy']:<10.4f} {perf['correct']}/{perf['total']}\")\n",
    "        \n",
    "        print(f\"\\n最佳表現分析：\")\n",
    "        if model_performances:\n",
    "            # 按準確率排序\n",
    "            model_performances.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "            best_accuracy = model_performances[0]\n",
    "            \n",
    "            # 按覆蓋率排序\n",
    "            best_coverage = max(model_performances, key=lambda x: x['coverage'])\n",
    "                   \n",
    "            # 按訓練時間排序\n",
    "            fastest_training = min(model_performances, key=lambda x: x['training_time'])\n",
    "            \n",
    "            print(f\"最高準確率: {best_accuracy['name']} ({best_accuracy['accuracy']:.4f})\")\n",
    "            print(f\"最高覆蓋率: {best_coverage['name']} ({best_coverage['coverage']:.4f})\")\n",
    "            print(f\"最快訓練: {fastest_training['name']} ({fastest_training['training_time']:.1f}s)\")\n",
    "        \n",
    "        print(f\"\\n報告生成完成!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator = MultiCorpusEvaluator(\n",
    "        \"questions-words.csv\",\n",
    "        max_corpus_size=80000,    \n",
    "        eval_sample_size=3000     \n",
    "    )\n",
    "    evaluator.run_comprehensive_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
